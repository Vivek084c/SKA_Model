{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-saved MNIST subset (100 samples per class)\n",
    "mnist_subset = torch.load(\"mnist_subset_100_per_class.pt\")\n",
    "images = torch.stack([item[0] for item in mnist_subset])  # Shape: [1000, 1, 28, 28]\n",
    "labels = torch.tensor([item[1] for item in mnist_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape # 1000 image each of size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset (single batch for SKA forward learning)\n",
    "inputs = images  # No mini-batches, full dataset used for forward-only updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the SKA model with 4 layers\n",
    "class SKAModel(nn.Module):\n",
    "    def __init__(self, input_size=784, layer_sizes=[256, 128, 64, 10], K=50):\n",
    "        super(SKAModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.K = K  # Number of forward steps\n",
    "\n",
    "        # Initialize weights and biases as nn.ParameterList\n",
    "        self.weights = nn.ParameterList()\n",
    "        self.biases = nn.ParameterList()\n",
    "        prev_size = input_size\n",
    "        for size in layer_sizes:\n",
    "            self.weights.append(nn.Parameter(torch.randn(prev_size, size) * 0.01))\n",
    "            self.biases.append(nn.Parameter(torch.zeros(size)))\n",
    "            prev_size = size\n",
    "\n",
    "        # Tracking tensors for knowledge accumulation and entropy computation\n",
    "        self.Z = [None] * len(layer_sizes)  # Knowledge tensors per layer\n",
    "        self.D = [None] * len(layer_sizes)  # Decision probability tensors\n",
    "        self.D_prev = [None] * len(layer_sizes)  # Previous decisions for computing shifts\n",
    "        self.delta_D = [None] * len(layer_sizes)  # Decision shifts per step\n",
    "        self.entropy = [None] * len(layer_sizes)  # Layer-wise entropy storage\n",
    "\n",
    "        # Store entropy, cosine, and output distribution history for visualization\n",
    "        self.entropy_history = [[] for _ in range(len(layer_sizes))]\n",
    "        self.cosine_history = [[] for _ in range(len(layer_sizes))]\n",
    "        self.output_history = []  # New: Store mean output distribution (10 classes) per step\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes SKA forward pass, storing knowledge and decisions.\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)  # Flatten images\n",
    "\n",
    "        for l in range(len(self.layer_sizes)):\n",
    "            # Compute knowledge tensor Z = Wx + b\n",
    "            z = torch.mm(x, self.weights[l]) + self.biases[l]\n",
    "            # Apply sigmoid activation to get decision probabilities\n",
    "            d = torch.sigmoid(z)\n",
    "            # Store values for entropy computation\n",
    "            self.Z[l] = z\n",
    "            self.D[l] = d\n",
    "            x = d  # Output becomes input for the next layer\n",
    "            \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def calculate_entropy(self):\n",
    "        \"\"\"Computes entropy reduction and cos(theta) per layer.\"\"\"\n",
    "        total_entropy = 0\n",
    "        for l in range(len(self.layer_sizes)):\n",
    "            if self.Z[l] is not None and self.D_prev[l] is not None and self.D[l] is not None:\n",
    "                # Compute decision shifts\n",
    "                self.delta_D[l] = self.D[l] - self.D_prev[l]\n",
    "                # Entropy reduction using SKA formula\n",
    "                dot_product = torch.sum(self.Z[l] * self.delta_D[l])\n",
    "                layer_entropy = -1 / np.log(2) * dot_product\n",
    "                self.entropy[l] = layer_entropy.item()\n",
    "                self.entropy_history[l].append(layer_entropy.item())\n",
    "\n",
    "                # Compute cos(theta) for alignment\n",
    "                z_norm = torch.norm(self.Z[l])\n",
    "                delta_d_norm = torch.norm(self.delta_D[l])\n",
    "                if z_norm > 0 and delta_d_norm > 0:\n",
    "                    cos_theta = dot_product / (z_norm * delta_d_norm)\n",
    "                    self.cosine_history[l].append(cos_theta.item())\n",
    "                else:\n",
    "                    self.cosine_history[l].append(0.0)  # Default if norms are zero\n",
    "\n",
    "                total_entropy += layer_entropy\n",
    "        return total_entropy\n",
    "\n",
    "\n",
    "    def ska_update(self, inputs, learning_rate=0.01):\n",
    "        \"\"\"Updates weights using entropy-based learning without backpropagation.\"\"\"\n",
    "        for l in range(len(self.layer_sizes)):\n",
    "            if self.delta_D[l] is not None:\n",
    "                # Previous layer's output\n",
    "                prev_output = inputs.view(inputs.shape[0], -1) if l == 0 else self.D_prev[l-1]\n",
    "                # Compute sigmoid derivative: D * (1 - D)\n",
    "                d_prime = self.D[l] * (1 - self.D[l])\n",
    "                # Compute entropy gradient\n",
    "                gradient = -1 / np.log(2) * (self.Z[l] * d_prime + self.delta_D[l])\n",
    "                # Compute weight updates via outer product\n",
    "                dW = torch.matmul(prev_output.t(), gradient) / prev_output.shape[0]\n",
    "                # Update weights and biases\n",
    "                self.weights[l] = self.weights[l] - learning_rate * dW\n",
    "                self.biases[l] = self.biases[l] - learning_rate * gradient.mean(dim=0)\n",
    "\n",
    "    def initialize_tensors(self, batch_size):\n",
    "        \"\"\"Resets decision tensors at the start of each training iteration.\"\"\"\n",
    "        for l in range(len(self.layer_sizes)):\n",
    "            self.Z[l] = None         # Reset knowledge tensors\n",
    "            self.D[l] = None         # Reset current decision probabilities\n",
    "            self.D_prev[l] = None    # Reset previous decision probabilities\n",
    "            self.delta_D[l] = None   # Reset decision shifts\n",
    "            self.entropy[l] = None   # Reset entropy storage\n",
    "            self.entropy_history[l] = []  # Reset entropy history\n",
    "            self.cosine_history[l] = []   # Reset cosine history\n",
    "        self.output_history = []  # Reset output history\n",
    "\n",
    "\n",
    "    def visualize_entropy_heatmap(self, step):\n",
    "        \"\"\"Dynamically scales the heatmap range and visualizes entropy reduction.\"\"\"\n",
    "        entropy_data = np.array(self.entropy_history)\n",
    "        vmin = np.min(entropy_data)  # Dynamically set minimum entropy value\n",
    "        vmax = 0.0  # Keep 0 as the upper limit for standardization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(entropy_data, cmap=\"Blues_r\", vmin=vmin, vmax=vmax,  \n",
    "                    xticklabels=range(1, entropy_data.shape[1] + 1),\n",
    "                    yticklabels=[f\"Layer {i+1}\" for i in range(len(self.layer_sizes))])\n",
    "        plt.title(f\"Layer-wise Entropy Heatmap (Step {step})\")\n",
    "        plt.xlabel(\"Step Index K\")\n",
    "        plt.ylabel(\"Network Layers\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"entropy_heatmap_step_{step}.png\")\n",
    "        plt.show(block=False)  # Non-blocking\n",
    "        plt.pause(2)  # Wait for 2 seconds\n",
    "        plt.close()  # Close automatically\n",
    "\n",
    "    def visualize_cosine_heatmap(self, step):\n",
    "        \"\"\"Visualizes cos(theta) alignment heatmap with a diverging scale.\"\"\"\n",
    "        cosine_data = np.array(self.cosine_history)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cosine_data, cmap=\"coolwarm_r\", vmin=-1.0, vmax=1.0,  \n",
    "                    xticklabels=range(1, cosine_data.shape[1] + 1),\n",
    "                    yticklabels=[f\"Layer {i+1}\" for i in range(len(self.layer_sizes))])\n",
    "        plt.title(f\"Layer-wise Cos(\\u03B8) Alignment Heatmap (Step {step})\")\n",
    "        plt.xlabel(\"Step Index K\")\n",
    "        plt.ylabel(\"Network Layers\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"cosine_heatmap_step_{step}.png\")\n",
    "        plt.show(block=False)  # Non-blocking\n",
    "        plt.pause(2)  # Wait for 2 seconds\n",
    "        plt.close()  # Close automatically\n",
    "\n",
    "    def visualize_output_distribution(self):\n",
    "        \"\"\"Plots the evolution of the 10-class output distribution over K steps.\"\"\"\n",
    "        output_data = np.array(self.output_history)  # Shape: [K, 10]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(output_data)  # Plot each class as a line\n",
    "        plt.title('Output Decision Probability Evolution Across Steps (Single Pass)')\n",
    "        plt.xlabel('Step Index K')\n",
    "        plt.ylabel('Mean Sigmoid Output')\n",
    "        plt.legend([f\"Class {i}\" for i in range(10)], loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"output_distribution_single_pass.png\")\n",
    "        plt.show(block=False)  # Non-blocking\n",
    "        plt.pause(2)  # Wait for 2 seconds\n",
    "        plt.close()  # Close automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward(self, x):\n",
    "    \"\"\"Computes SKA forward pass, storing knowledge and decisions.\"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    x = x.view(batch_size, -1)  # Flatten images\n",
    "\n",
    "    for l in range(len(self.layer_sizes)):\n",
    "        # Compute knowledge tensor Z = Wx + b\n",
    "        z = torch.mm(x, self.weights[l]) + self.biases[l]\n",
    "        # Apply sigmoid activation to get decision probabilities\n",
    "        d = torch.sigmoid(z)\n",
    "        # Store values for entropy computation\n",
    "        self.Z[l] = z\n",
    "        self.D[l] = d\n",
    "        x = d  # Output becomes input for the next layer\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "model = SKAModel()\n",
    "learning_rate = 0.01\n",
    "\n",
    "# SKA training over multiple forward steps\n",
    "total_entropy = 0\n",
    "step_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize tensors for first step\n",
    "# model.initialize_tensors(inputs.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[0] # we get the number of inputsinputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.view(inputs.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4990, 0.5044, 0.4918,  ..., 0.4899, 0.4883, 0.4966],\n",
       "        [0.4990, 0.5044, 0.4918,  ..., 0.4899, 0.4883, 0.4966],\n",
       "        [0.4990, 0.5044, 0.4918,  ..., 0.4899, 0.4883, 0.4966],\n",
       "        ...,\n",
       "        [0.4990, 0.5044, 0.4918,  ..., 0.4899, 0.4883, 0.4966],\n",
       "        [0.4990, 0.5044, 0.4918,  ..., 0.4899, 0.4883, 0.4966],\n",
       "        [0.4990, 0.5044, 0.4918,  ..., 0.4899, 0.4883, 0.4966]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4990, 0.5044, 0.4918, 0.5003, 0.4952, 0.5106, 0.5084, 0.4899, 0.4883,\n",
       "        0.4966], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4990, 0.5044, 0.4918, 0.5003, 0.4952, 0.5106, 0.5084, 0.4899, 0.4883,\n",
       "        0.4966], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ska_update(inputs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_entropy = model.calculate_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f'{batch_entropy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ska_update(inputs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.D_prev = [d.clone().detach() if d is not None else None for d in model.D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Total Steps: 1, Entropy: 0.0000\n",
      "Step: 2, Total Steps: 2, Entropy: -143.1017\n",
      "Step: 3, Total Steps: 3, Entropy: -236.6775\n",
      "Step: 4, Total Steps: 4, Entropy: -359.4742\n",
      "Step: 5, Total Steps: 5, Entropy: -543.4399\n",
      "Step: 6, Total Steps: 6, Entropy: -821.6418\n",
      "Step: 7, Total Steps: 7, Entropy: -1235.7252\n",
      "Step: 8, Total Steps: 8, Entropy: -1834.0430\n",
      "Step: 9, Total Steps: 9, Entropy: -2662.0410\n",
      "Step: 10, Total Steps: 10, Entropy: -3744.1326\n",
      "Step: 11, Total Steps: 11, Entropy: -5061.6333\n",
      "Step: 12, Total Steps: 12, Entropy: -6539.8579\n",
      "Step: 13, Total Steps: 13, Entropy: -8057.0767\n",
      "Step: 14, Total Steps: 14, Entropy: -9475.7354\n",
      "Step: 15, Total Steps: 15, Entropy: -10686.1582\n",
      "Step: 16, Total Steps: 16, Entropy: -11644.7881\n",
      "Step: 17, Total Steps: 17, Entropy: -12383.8447\n",
      "Step: 18, Total Steps: 18, Entropy: -12982.5801\n",
      "Step: 19, Total Steps: 19, Entropy: -13512.2471\n",
      "Step: 20, Total Steps: 20, Entropy: -13987.2754\n",
      "Step: 21, Total Steps: 21, Entropy: -14362.8779\n",
      "Step: 22, Total Steps: 22, Entropy: -14584.2139\n",
      "Step: 23, Total Steps: 23, Entropy: -14634.5850\n",
      "Step: 24, Total Steps: 24, Entropy: -14536.8086\n",
      "Step: 25, Total Steps: 25, Entropy: -14315.8105\n",
      "Step: 26, Total Steps: 26, Entropy: -13968.8115\n",
      "Step: 27, Total Steps: 27, Entropy: -13480.9434\n",
      "Step: 28, Total Steps: 28, Entropy: -12859.2148\n",
      "Step: 29, Total Steps: 29, Entropy: -12145.5078\n",
      "Step: 30, Total Steps: 30, Entropy: -11404.5498\n",
      "Step: 31, Total Steps: 31, Entropy: -10690.8604\n",
      "Step: 32, Total Steps: 32, Entropy: -10027.6943\n",
      "Step: 33, Total Steps: 33, Entropy: -9418.8857\n",
      "Step: 34, Total Steps: 34, Entropy: -8864.6562\n",
      "Step: 35, Total Steps: 35, Entropy: -8364.5332\n",
      "Step: 36, Total Steps: 36, Entropy: -7915.6865\n",
      "Step: 37, Total Steps: 37, Entropy: -7512.3691\n",
      "Step: 38, Total Steps: 38, Entropy: -7145.9648\n",
      "Step: 39, Total Steps: 39, Entropy: -6806.1440\n",
      "Step: 40, Total Steps: 40, Entropy: -6483.8018\n",
      "Step: 41, Total Steps: 41, Entropy: -6172.7637\n",
      "Step: 42, Total Steps: 42, Entropy: -5868.6025\n",
      "Step: 43, Total Steps: 43, Entropy: -5568.7334\n",
      "Step: 44, Total Steps: 44, Entropy: -5274.1123\n",
      "Step: 45, Total Steps: 45, Entropy: -4987.9507\n",
      "Step: 46, Total Steps: 46, Entropy: -4712.5952\n",
      "Step: 47, Total Steps: 47, Entropy: -4448.5166\n",
      "Step: 48, Total Steps: 48, Entropy: -4195.1914\n",
      "Step: 49, Total Steps: 49, Entropy: -3952.0889\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "model = SKAModel()\n",
    "learning_rate = 0.01\n",
    "\n",
    "# SKA training over multiple forward steps\n",
    "total_entropy = 0\n",
    "step_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize tensors for first step\n",
    "model.initialize_tensors(inputs.size(0))\n",
    "\n",
    "# Process K forward steps (without backpropagation)\n",
    "for k in range(model.K):\n",
    "    outputs = model.forward(inputs)\n",
    "    # Store mean output distribution for the final layer\n",
    "    model.output_history.append(outputs.mean(dim=0).detach().cpu().numpy())  # [10] vector\n",
    "    if k > 0:  # Compute entropy after first step\n",
    "        batch_entropy = model.calculate_entropy()\n",
    "        model.ska_update(inputs, learning_rate)\n",
    "        total_entropy += batch_entropy\n",
    "        step_count += 1\n",
    "        print(f'Step: {k}, Total Steps: {step_count}, Entropy: {batch_entropy:.4f}')\n",
    "        # model.visualize_entropy_heatmap(step_count)\n",
    "        # model.visualize_cosine_heatmap(step_count)  # Add cosine heatmap\n",
    "    # Update previous decision tensors\n",
    "    model.D_prev = [d.clone().detach() if d is not None else None for d in model.D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
